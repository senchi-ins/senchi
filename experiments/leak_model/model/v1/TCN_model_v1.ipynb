{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfec3f49",
   "metadata": {},
   "source": [
    "# TCN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794b4fe",
   "metadata": {},
   "source": [
    "To get simulation data run `./run_sim.sh [small, medium, large, test or custom]` from the `leak_model` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af71737e",
   "metadata": {},
   "source": [
    "Then run `gunzip [file_path]` in your terminal to turn the gz file output from the sim into a csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36ea06",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import qexpy as q\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import MissingValuesFiller\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import TCNModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb266c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../sim_data/synthetic_water_data_minute_1000.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b4b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e1ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc30fc6",
   "metadata": {},
   "source": [
    "## Leak df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbff6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01967cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_cols = ['leak_category','leak_branch','leak_pipe']\n",
    "leak_df[nan_cols] = leak_df[nan_cols].fillna(value='none')\n",
    "leak_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b1c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a59c9d1",
   "metadata": {},
   "source": [
    "## Calculating velocity and flow erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a99cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_errors(df):\n",
    "    theta_deg = 60\n",
    "    theta_cos = np.cos(np.radians(theta_deg))\n",
    "    epsilon = 1e-10\n",
    "\n",
    "    # Extract needed fields as arrays\n",
    "    L = df['l_path_m'].values\n",
    "    t_u = df['upstream_transit_time_s'].values\n",
    "    t_d = df['downstream_transit_time_s'].values\n",
    "    V_actual = df['velocity_m_per_s'].values\n",
    "    Q_actual = df['flow_m3_s'].values\n",
    "    id_m = df['id_mm'].values / 1000  # mm to m\n",
    "\n",
    "    # Step 1: Estimate velocity\n",
    "    V_est = (L / (2 * theta_cos)) * ((1 / t_d) - (1 / t_u))\n",
    "\n",
    "    # Step 2: Area of circular pipe\n",
    "    A = (np.pi/4) * (id_m ** 2)\n",
    "\n",
    "    # Step 3: Estimate flow\n",
    "    Q_est = V_est * A\n",
    "\n",
    "    # Step 4: Error metrics\n",
    "    velocity_error = np.abs(V_actual - V_est) / np.maximum(np.abs(V_actual), epsilon)\n",
    "    flow_rate_error = np.abs(Q_actual - Q_est) / np.maximum(np.abs(Q_actual), epsilon)\n",
    "\n",
    "    # Step 5: Add to DataFrame\n",
    "    df['V_est'] = V_est\n",
    "    df['Q_est'] = Q_est\n",
    "    df['velocity_error'] = velocity_error\n",
    "    df['flow_rate_error'] = flow_rate_error\n",
    "    df['velocity_error_pass'] = velocity_error <= 0.05\n",
    "    df['flow_rate_error_pass'] = flow_rate_error <= 0.05\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171312b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply error calculations to the dataframe\n",
    "calculate_errors(leak_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a67580",
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfe8ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_df[leak_df['pipe_burst_leak'] == True]['house_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a779f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_df[leak_df['pipe_burst_leak'] == True]['leak_pipe'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f78a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "houses = [100]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot flow data for each random house\n",
    "for house_id in houses:\n",
    "    house_data = leak_df[leak_df['house_id'] == house_id].copy()\n",
    "    house_data['timestamp'] = pd.to_datetime(house_data['timestamp'])\n",
    "    house_data = house_data.sort_values('timestamp')\n",
    "    \n",
    "    plt.plot(house_data['timestamp'], house_data['flow_m3_s'], \n",
    "             label=f'House {house_id}', alpha=0.7, linewidth=1)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Flow Rate (m³/s)')\n",
    "plt.title('Flow Rate Over Time a house')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the selected house IDs\n",
    "print(f\"Selected house IDs: {sorted(houses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9f4f7c",
   "metadata": {},
   "source": [
    "# TCN development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3707ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_eng_df = leak_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fed41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining features and targets\n",
    "NUMERICAL_FEATURES_v1 = [\n",
    "    'velocity_m_per_s', 'flow_m3_s', 'upstream_transit_time_s', \n",
    "    'downstream_transit_time_s', 'delta_t_ns', 'pipe_width_in',\n",
    "    'od_mm', 'wall_mm', 'id_mm', 'c_est_m_per_s', 'temp_est_c'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES_v1 = [\n",
    "    'pipe_material', 'leak_type', 'leak_category', 'leak_pipe'\n",
    "]\n",
    "\n",
    "TARGET_COLUMNS = ['pipe_burst_leak', 'leak_branch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d30135",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_numerical = [col for col in NUMERICAL_FEATURES_v1 if col not in feat_eng_df.columns]\n",
    "missing_categorical = [col for col in CATEGORICAL_FEATURES_v1 if col not in feat_eng_df.columns]\n",
    "missing_targets = [col for col in TARGET_COLUMNS if col not in feat_eng_df.columns]\n",
    "\n",
    "if missing_numerical:\n",
    "    print(f\"Missing numerical features: {missing_numerical}\")\n",
    "if missing_categorical:\n",
    "    print(f\"Missing categorical features: {missing_categorical}\")\n",
    "if missing_targets:\n",
    "    print(f\"Missing target columns: {missing_targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112dbaf6",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0316a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_encodings(df):\n",
    "    \"\"\"Create one-hot encodings for categorical features\"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    for cat_feature in CATEGORICAL_FEATURES_v1:\n",
    "        if cat_feature in df_encoded.columns:\n",
    "            # Get unique values\n",
    "            unique_vals = df_encoded[cat_feature].unique()\n",
    "            print(f\"{cat_feature} unique values: {unique_vals}\")\n",
    "            \n",
    "            # Create one-hot encoding\n",
    "            encoded_cols = pd.get_dummies(df_encoded[cat_feature], \n",
    "                                        prefix=f'{cat_feature}_onehot', \n",
    "                                        prefix_sep='_')\n",
    "            \n",
    "            # Add encoded columns to dataframe\n",
    "            df_encoded = pd.concat([df_encoded, encoded_cols], axis=1)\n",
    "            \n",
    "            print(f\"Created {len(encoded_cols.columns)} columns for {cat_feature}\")\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146bf558",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_eng_df = create_feature_encodings(feat_eng_df)\n",
    "feat_eng_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a33c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_categorical_targets(df, target_cols):\n",
    "    \"\"\"Prepare both binary and categorical targets\"\"\"\n",
    "    df_targets = df.copy()\n",
    "    \n",
    "    for target_col in target_cols:\n",
    "        if target_col in df_targets.columns:\n",
    "            unique_vals = df_targets[target_col].unique()\n",
    "            \n",
    "            if target_col == 'pipe_burst_leak':\n",
    "                # Binary target - keep as is (boolean/binary)\n",
    "                print(f\"{target_col}: Binary target, keeping as boolean\")\n",
    "                \n",
    "            elif target_col == 'leak_branch':\n",
    "                # Categorical target - we have options:\n",
    "                print(f\"{target_col} unique values: {unique_vals}\")\n",
    "                \n",
    "                # One-hot encode for multi-output\n",
    "                target_encoded = pd.get_dummies(df_targets[target_col], \n",
    "                                              prefix=f'{target_col}_onehot', \n",
    "                                              prefix_sep='_')\n",
    "                df_targets = pd.concat([df_targets, target_encoded], axis=1)\n",
    "                print(f\"One-hot encoded {target_col}: {target_encoded.columns.tolist()}\")\n",
    "    \n",
    "    return df_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759cc049",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_eng_df = prepare_categorical_targets(feat_eng_df, TARGET_COLUMNS)\n",
    "feat_eng_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ba5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_eng_df.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458cedb2",
   "metadata": {},
   "source": [
    "### Forward shifting targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_supervised_learning_targets(df, forecast_horizon: int = 24): # 24 15-min sections = 6 hours\n",
    "    \"\"\"\n",
    "    Create supervised learning targets for the TCN model. Currently, this \n",
    "    creates a prediction if there will be any leaks in the next 24 hours.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_with_targets = df.copy()\n",
    "    df_with_targets[\"timestamp\"] = pd.to_datetime(df_with_targets[\"timestamp\"])\n",
    "    df_with_targets = df_with_targets.sort_values([\"house_id\", \"timestamp\"])\n",
    "    \n",
    "    future_targets = []\n",
    "    \n",
    "    for house_id in df_with_targets[\"house_id\"].unique():\n",
    "        house_data = df_with_targets[df_with_targets[\"house_id\"] == house_id].copy()\n",
    "        \n",
    "        # Rolling window - predict if leak occurs anywhere in next 24 hours\n",
    "        house_data[\"pipe_burst_leak_next_24h\"] = (\n",
    "            house_data[\"pipe_burst_leak\"]\n",
    "            .rolling(window=forecast_horizon, min_periods=1)\n",
    "            .max()\n",
    "            .shift(-forecast_horizon + 1)\n",
    "            .fillna(False)\n",
    "            .astype(bool)\n",
    "        )\n",
    "        \n",
    "        onehot_cols = [col for col in house_data.columns if col.startswith(\"leak_branch_onehot_\")]\n",
    "        \n",
    "        for col in onehot_cols:\n",
    "            # Only mark branch as True if leak happens on that specific branch\n",
    "            leak_on_this_branch = (house_data[col] & house_data[\"pipe_burst_leak\"])\n",
    "            \n",
    "            house_data[f\"{col}_next_24h\"] = (\n",
    "                leak_on_this_branch\n",
    "                .rolling(window=forecast_horizon, min_periods=1)\n",
    "                .max()\n",
    "                .shift(-forecast_horizon + 1)\n",
    "                .fillna(False)\n",
    "                .astype(bool)\n",
    "            )\n",
    "        \n",
    "        # Handle \"none\" case - True only when no leak predicted at all\n",
    "        none_flag = f\"leak_branch_onehot_none_next_24h\"\n",
    "        if none_flag in house_data.columns:\n",
    "            house_data[none_flag] = (\n",
    "                house_data[\"pipe_burst_leak_next_24h\"] == False\n",
    "            )\n",
    "        \n",
    "        future_targets.append(house_data)\n",
    "    \n",
    "    return pd.concat(future_targets, axis=0, ignore_index=True).dropna(\n",
    "        subset=[\"pipe_burst_leak_next_24h\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ebfa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_eng_df = create_supervised_learning_targets(feat_eng_df)\n",
    "feat_eng_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d5e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_eng_df[feat_eng_df['pipe_burst_leak_next_24h'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbafac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_eng_df[(feat_eng_df['pipe_burst_leak_next_24h'] == True) & (feat_eng_df['leak_branch_onehot_POWDER_ROOM_BRANCH_next_24h'] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7495a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_branch_24h_cols = [col for col in feat_eng_df.columns if col.startswith('leak_branch_onehot_') and col.endswith('_next_24h')]\n",
    "feat_eng_df[(feat_eng_df['pipe_burst_leak_next_24h']) & (feat_eng_df[leak_branch_24h_cols].eq(False).any(axis=1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc4da8",
   "metadata": {},
   "source": [
    "### Final feature summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e4de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = feat_eng_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_tcn_features_final(df_final):\n",
    "    \"\"\"Final summary with target options (updated for _next_24h convention)\"\"\"\n",
    "    \n",
    "    metadata_cols = ['timestamp', 'house_id'] + CATEGORICAL_FEATURES_v1 + TARGET_COLUMNS\n",
    "    \n",
    "    feature_cols = [\n",
    "        col for col in df_final.columns\n",
    "        if col not in metadata_cols\n",
    "        and not col.endswith(('_next_24h', 'pressure_psi'))\n",
    "        and not col.startswith('leak_branch_onehot_')\n",
    "        and not col.startswith(('V_est', 'Q_est'))\n",
    "        and not col.endswith(('_error', '_pass'))\n",
    "    ]\n",
    "\n",
    "    onehot_targets = [c for c in df_final.columns\n",
    "                     if c.startswith('leak_branch_onehot_') and c.endswith('_next_24h')]\n",
    "\n",
    "    summary = {\n",
    "        'rows': len(df_final),\n",
    "        'num_features': len(feature_cols),\n",
    "        'feature_columns': feature_cols,\n",
    "        'binary_target': 'pipe_burst_leak_next_24h',\n",
    "        'categorical_targets': onehot_targets,\n",
    "        'num_categorical_targets': len(onehot_targets)\n",
    "    }\n",
    "    \n",
    "    print(\"=== FINAL TCN DATA SUMMARY ===\")\n",
    "    print(f\"Rows: {len(df_final):,}\")\n",
    "    print(f\"Features: {len(feature_cols)}\")\n",
    "    print(\"Target options:\")\n",
    "    print(\"  - Binary: pipe_burst_leak_next_24h\")\n",
    "    print(f\"  - Categorical (one-hot): {len(onehot_targets)} columns\")\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8603fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_tcn_features_final(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87a478",
   "metadata": {},
   "source": [
    "### Target distribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2d3f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Distribution Validation\n",
    "print(\"=\"*60)\n",
    "print(\"TARGET DISTRIBUTION VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. OVERALL LEAK STATISTICS\n",
    "print(\"\\n1. OVERALL LEAK STATISTICS\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "total_samples = len(final_df)\n",
    "leak_samples = final_df['pipe_burst_leak'].sum()\n",
    "leak_rate = leak_samples / total_samples\n",
    "\n",
    "print(f\"Total samples: {total_samples:,}\")\n",
    "print(f\"Leak samples: {leak_samples:,}\")\n",
    "print(f\"No-leak samples: {total_samples - leak_samples:,}\")\n",
    "print(f\"Overall leak rate: {leak_rate:.4f} ({leak_rate*100:.2f}%)\")\n",
    "print(f\"Class imbalance ratio: {(total_samples - leak_samples) / leak_samples:.1f}:1 (no-leak:leak)\")\n",
    "\n",
    "# 2. PER-HOUSE LEAK DISTRIBUTION\n",
    "print(\"\\n2. PER-HOUSE LEAK DISTRIBUTION\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "house_leak_stats = []\n",
    "for house_id in sorted(final_df['house_id'].unique()):\n",
    "    house_data = final_df[final_df['house_id'] == house_id]\n",
    "    house_total = len(house_data)\n",
    "    house_leaks = house_data['pipe_burst_leak'].sum()\n",
    "    house_leak_rate = house_leaks / house_total if house_total > 0 else 0\n",
    "    \n",
    "    house_leak_stats.append({\n",
    "        'house_id': house_id,\n",
    "        'total_samples': house_total,\n",
    "        'leak_samples': house_leaks,\n",
    "        'leak_rate': house_leak_rate\n",
    "    })\n",
    "\n",
    "house_stats_df = pd.DataFrame(house_leak_stats)\n",
    "\n",
    "print(f\"Houses with leaks: {(house_stats_df['leak_samples'] > 0).sum()}\")\n",
    "print(f\"Houses without leaks: {(house_stats_df['leak_samples'] == 0).sum()}\")\n",
    "print(f\"Average leak rate per house: {house_stats_df['leak_rate'].mean():.4f}\")\n",
    "print(f\"Std leak rate per house: {house_stats_df['leak_rate'].std():.4f}\")\n",
    "print(f\"Min leak rate: {house_stats_df['leak_rate'].min():.4f}\")\n",
    "print(f\"Max leak rate: {house_stats_df['leak_rate'].max():.4f}\")\n",
    "\n",
    "# 3. LEAK RATE THRESHOLD ANALYSIS (20%)\n",
    "print(\"\\n3. LEAK RATE THRESHOLD ANALYSIS (20%)\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "threshold = 0.15  # 20%\n",
    "houses_over_threshold = house_stats_df[house_stats_df['leak_rate'] > threshold]\n",
    "houses_at_or_below_threshold = house_stats_df[house_stats_df['leak_rate'] <= threshold]\n",
    "\n",
    "print(f\"Houses with leak rate > 15%: {len(houses_over_threshold)}\")\n",
    "print(f\"Houses with leak rate ≤ 15%: {len(houses_at_or_below_threshold)}\")\n",
    "print(f\"Percentage of houses > 15%: {len(houses_over_threshold) / len(house_stats_df) * 100:.2f}%\")\n",
    "print(f\"Percentage of houses ≤ 15%: {len(houses_at_or_below_threshold) / len(house_stats_df) * 100:.2f}%\")\n",
    "\n",
    "# Show houses with highest leak rates\n",
    "print(f\"\\nTop 10 houses by leak rate:\")\n",
    "top_leak_houses = house_stats_df.nlargest(10, 'leak_rate')\n",
    "for _, row in top_leak_houses.iterrows():\n",
    "    print(f\"House {row['house_id']}: {row['leak_samples']}/{row['total_samples']} = {row['leak_rate']:.4f}\")\n",
    "\n",
    "# Show houses over 20% threshold\n",
    "if len(houses_over_threshold) > 0:\n",
    "    print(f\"\\nHouses with leak rate > 15%:\")\n",
    "    for _, row in houses_over_threshold.iterrows():\n",
    "        print(f\"House {row['house_id']}: {row['leak_samples']}/{row['total_samples']} = {row['leak_rate']:.4f}\")\n",
    "else:\n",
    "    print(f\"\\nNo houses have leak rate > 15%\")\n",
    "\n",
    "# 4. CREATE FILTERED DATAFRAME\n",
    "print(\"\\n4. CREATE FILTERED DATAFRAME\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Get house IDs to keep (those with leak rate <= threshold)\n",
    "houses_to_keep = houses_at_or_below_threshold['house_id'].tolist()\n",
    "\n",
    "# Create filtered DataFrame\n",
    "ff_df = final_df[final_df['house_id'].isin(houses_to_keep)].copy()\n",
    "\n",
    "print(f\"Original dataset size: {len(final_df):,} rows\")\n",
    "print(f\"Filtered dataset size: {len(ff_df):,} rows\")\n",
    "print(f\"Removed {len(final_df) - len(ff_df):,} rows\")\n",
    "print(f\"Removed {len(houses_over_threshold)} houses with leak rate > {threshold*100}%\")\n",
    "\n",
    "# Verify the filtering worked correctly\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Remaining houses: {ff_df['house_id'].nunique()}\")\n",
    "print(f\"Removed houses: {len(houses_over_threshold)}\")\n",
    "\n",
    "# Check leak rate in filtered dataset\n",
    "filtered_total = len(ff_df)\n",
    "filtered_leaks = ff_df['pipe_burst_leak'].sum()\n",
    "filtered_leak_rate = filtered_leaks / filtered_total if filtered_total > 0 else 0\n",
    "\n",
    "print(f\"Filtered dataset leak rate: {filtered_leak_rate:.4f} ({filtered_leak_rate*100:.2f}%)\")\n",
    "print(f\"Original dataset leak rate: {leak_rate:.4f} ({leak_rate*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ed8c8",
   "metadata": {},
   "source": [
    "### Features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "    'velocity_m_per_s', 'flow_m3_s', 'upstream_transit_time_s',\n",
    "    'downstream_transit_time_s', 'delta_t_ns', 'pipe_width_in', 'od_mm',\n",
    "    'wall_mm', 'id_mm', 'l_path_m', 'c_est_m_per_s', 'temp_est_c',\n",
    "    'n_traverses', 'theta_deg', 'pipe_material_onehot_Copper',\n",
    "    'pipe_material_onehot_PEX', 'leak_type_onehot_burst_freeze',\n",
    "    'leak_type_onehot_burst_pressure', 'leak_type_onehot_gradual',\n",
    "    'leak_type_onehot_micro', 'leak_type_onehot_none',\n",
    "    'leak_category_onehot_dish', 'leak_category_onehot_faucet',\n",
    "    'leak_category_onehot_laundry', 'leak_category_onehot_none',\n",
    "    'leak_category_onehot_shower', 'leak_category_onehot_toilet',\n",
    "    'leak_category_onehot_unknown', 'leak_pipe_onehot_P_DISHWASHER',\n",
    "    'leak_pipe_onehot_P_ENS_LAV', 'leak_pipe_onehot_P_ENS_SHWR',\n",
    "    'leak_pipe_onehot_P_ENS_WC', 'leak_pipe_onehot_P_FAM_LAV',\n",
    "    'leak_pipe_onehot_P_FAM_TUB', 'leak_pipe_onehot_P_FAM_WC',\n",
    "    'leak_pipe_onehot_P_HOSE_BACK', 'leak_pipe_onehot_P_HOSE_FRONT',\n",
    "    'leak_pipe_onehot_P_KITCHEN_BRANCH', 'leak_pipe_onehot_P_KITCHEN_SINK',\n",
    "    'leak_pipe_onehot_P_LAUNDRY', 'leak_pipe_onehot_P_MAIN_1',\n",
    "    'leak_pipe_onehot_P_MAIN_2', 'leak_pipe_onehot_P_POWDER_BRANCH',\n",
    "    'leak_pipe_onehot_P_POWDER_LAV', 'leak_pipe_onehot_P_POWDER_WC',\n",
    "    'leak_pipe_onehot_P_UPPER_BRANCH', 'leak_pipe_onehot_P_WATER_HEATER',\n",
    "    'leak_pipe_onehot_none'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29123a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS = [\n",
    "    'pipe_burst_leak_next_24h',\n",
    "    'leak_branch_onehot_KITCHEN_BRANCH_next_24h',\n",
    "    'leak_branch_onehot_MAIN_TRUNK_1_next_24h',\n",
    "    'leak_branch_onehot_MAIN_TRUNK_2_next_24h',\n",
    "    'leak_branch_onehot_POWDER_ROOM_BRANCH_next_24h',\n",
    "    'leak_branch_onehot_UPPER_FLOOR_BRANCH_next_24h',\n",
    "    'leak_branch_onehot_none_next_24h',\n",
    "    'leak_branch_onehot_unknown_next_24h'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ebf9c",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0895274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "folder_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def scale_features_simple(df):\n",
    "    \"\"\"Simple sklearn-based scaling for numerical features\"\"\"\n",
    "    df_scaled = df.copy()\n",
    "\n",
    "    # Separate numerical and one-hot features\n",
    "    numerical_features = [\n",
    "        'velocity_m_per_s', 'flow_m3_s', 'flow_gpm', 'upstream_transit_time_s',\n",
    "        'downstream_transit_time_s', 'delta_t_ns', 'pipe_width_in', 'od_mm',\n",
    "        'wall_mm', 'id_mm', 'l_path_m', 'c_est_m_per_s', 'temp_est_c',\n",
    "        'n_traverses', 'theta_deg'\n",
    "    ]\n",
    "\n",
    "    # Fit scaler on numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled[numerical_features] = scaler.fit_transform(df_scaled[numerical_features])\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    scaler_path = f'weights/{folder_timestamp}/feature_scaler.pkl'\n",
    "    os.makedirs(os.path.dirname(scaler_path), exist_ok=True)\n",
    "    \n",
    "    # Save the scaler\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"Scaler saved to {scaler_path}\")\n",
    "\n",
    "    # Show scaling stats\n",
    "    for col in numerical_features[:5]:\n",
    "        print(f\"{col}: mean={df_scaled[col].mean():.3f}, std={df_scaled[col].std():.3f}\")\n",
    "\n",
    "    return df_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5dc23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_scaled, feature_scaler = scale_features_simple(ff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518395e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e388799",
   "metadata": {},
   "source": [
    "### Darts timeseries objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae08d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes_mask = ff_scaled.duplicated(subset=['house_id', 'timestamp'], keep=False)\n",
    "\n",
    "# How many duplicates overall\n",
    "print(f\"Total duplicate rows: {dupes_mask.sum():,}\")\n",
    "\n",
    "# Count duplicates per house (only the problematic ones)\n",
    "dupes_per_house = (\n",
    "    ff_scaled.loc[dupes_mask, ['house_id', 'timestamp']]\n",
    "    .groupby('house_id')\n",
    "    .size()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\nTop houses with duplicate timestamps:\")\n",
    "print(dupes_per_house.head(10))\n",
    "\n",
    "# Inspect a specific house to verify:\n",
    "sample_house = dupes_per_house.index[0] if not dupes_per_house.empty else None\n",
    "if sample_house is not None:\n",
    "    display(\n",
    "        ff_scaled[ff_scaled['house_id'] == sample_house]\n",
    "        .loc[lambda d: d.duplicated(subset='timestamp', keep=False)]\n",
    "        .sort_values('timestamp')\n",
    "        .head(20)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e972473",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_df = ff_scaled.copy()\n",
    "before = len(dup_df)\n",
    "\n",
    "dup_df = (\n",
    "    dup_df\n",
    "    .sort_values(['house_id', 'timestamp'])\n",
    "    .drop_duplicates(subset=['house_id', 'timestamp'],\n",
    "                     keep='first')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "after = len(dup_df)\n",
    "print(f\"Removed {before - after:,} duplicate rows \"\n",
    "      f\"({before:,} → {after:,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts = dup_df.copy()\n",
    "drop_cols = FEATURES + TARGETS\n",
    "df_ts[df_ts[drop_cols].isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any remaining NaNs in features/targets only\n",
    "df_ts = df_ts.dropna(subset=drop_cols)\n",
    "\n",
    "# Sort by house_id and timestamp\n",
    "df_ts['timestamp'] = pd.to_datetime(df_ts['timestamp'])\n",
    "df_ts = df_ts.sort_values(['house_id', 'timestamp'])\n",
    "\n",
    "# Convert targets and features to float32 for probability modelling\n",
    "df_ts[TARGETS] = df_ts[TARGETS].astype(\"float32\")\n",
    "df_ts[FEATURES] = df_ts[FEATURES].astype(\"float32\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b4124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c207d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a series per house\n",
    "house_feature_series   = {} # id -> TimeSeries of covariates\n",
    "house_target_series    = {} # id -> TimeSeries of targetsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d351a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hid, group in df_ts.groupby('house_id', sort=False):\n",
    "    # Time is the index\n",
    "    group = group.set_index('timestamp', drop=False)\n",
    "    \n",
    "    # Features\n",
    "    feat_ts = TimeSeries.from_dataframe(\n",
    "        group,\n",
    "        time_col='timestamp',\n",
    "        value_cols=FEATURES,\n",
    "        freq='15min',\n",
    "    ).with_static_covariates(pd.DataFrame({'house_id':[hid]}))\n",
    "    \n",
    "    # Targets\n",
    "    targ_ts = TimeSeries.from_dataframe(\n",
    "        group,\n",
    "        time_col='timestamp',\n",
    "        value_cols=TARGETS,\n",
    "        freq='15min',\n",
    "    ).with_static_covariates(pd.DataFrame({'house_id':[hid]}))\n",
    "    \n",
    "    # Store\n",
    "    house_feature_series[hid] = feat_ts\n",
    "    house_target_series[hid]  = targ_ts\n",
    "\n",
    "print(f\"Built {len(house_feature_series)} house TimeSeries objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4feb40",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a90aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import time, gc, json, warnings\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit # Chane to stratifiedgroupkfold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Darts\n",
    "from darts.models import TCNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a457cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dicts built earlier into reproducible ordered lists\n",
    "house_ids = np.array(sorted(house_feature_series.keys()))\n",
    "feat_series_list = [house_feature_series[h] for h in house_ids]\n",
    "targ_series_list = [house_target_series[h]  for h in house_ids]\n",
    "\n",
    "print(f\"Total houses: {len(house_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e599ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV configuration\n",
    "outer_cv = StratifiedKFold(n_splits=5) \n",
    "# inner_cv = TimeSeriesSplit(n_splits=6, test_size=96, gap=96) # Not used\n",
    "\n",
    "# TCN hyper-parameters\n",
    "INPUT_CHUNK  = 7 * 96 # 1 week history\n",
    "OUTPUT_CHUNK = 24 # 6h forecast horizon\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "NUM_LAYERS = 3\n",
    "NUM_FILTERS = 64\n",
    "KERNEL_SIZE = 3\n",
    "DILATION = 2\n",
    "DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec342e2",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f0ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.utils.likelihood_models.torch import BernoulliLikelihood\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    log_loss,\n",
    "    brier_score_loss,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "def run_outer_fold(train_idx, test_idx, fold_no):\n",
    "    \"\"\"\n",
    "    Train on `train_idx` houses, validate on the last OUTPUT_CHUNK of each\n",
    "    remaining house, and evaluate on `test_idx`.\n",
    "\n",
    "    Houses whose time-series are shorter than the minimum length required by\n",
    "    Darts (INPUT_CHUNK + OUTPUT_CHUNK) are skipped (“dropped out”).\n",
    "    \"\"\"\n",
    "    # Gather per-house series lists\n",
    "    train_feats_outer = [feat_series_list[i] for i in train_idx]\n",
    "    train_targ_outer = [targ_series_list[i] for i in train_idx]\n",
    "    test_feats_outer = [feat_series_list[i] for i in test_idx]\n",
    "    test_targ_outer = [targ_series_list[i] for i in test_idx]\n",
    "\n",
    "    # Minimum length of the portion fed to model.fit()\n",
    "    MIN_TRAIN_LEN = INPUT_CHUNK + OUTPUT_CHUNK # 7 days + 24 h = 768\n",
    "\n",
    "    # Build training / validation sets, dropping houses with too few points\n",
    "    train_feats, train_targs, val_feats, val_targs = [], [], [], []\n",
    "    dropped = 0\n",
    "\n",
    "    for feats_ts, targ_ts in zip(train_feats_outer, train_targ_outer):\n",
    "        n_points = len(targ_ts)\n",
    "\n",
    "        # Need MIN_TRAIN_LEN for fitting + OUTPUT_CHUNK for validation chunk\n",
    "        if n_points < MIN_TRAIN_LEN + OUTPUT_CHUNK:\n",
    "            dropped += 1\n",
    "            continue\n",
    "        \n",
    "        VAL_WINDOW = INPUT_CHUNK + OUTPUT_CHUNK \n",
    "        split_idx = n_points - VAL_WINDOW\n",
    "        train_feats.append(feats_ts[:split_idx])\n",
    "        train_targs.append(targ_ts[:split_idx])\n",
    "        val_feats.append(feats_ts[split_idx:])\n",
    "        val_targs.append(targ_ts[split_idx:])\n",
    "\n",
    "    if dropped:\n",
    "        print(f\"Dropped {dropped} train houses that were < \"\n",
    "              f\"{MIN_TRAIN_LEN + OUTPUT_CHUNK} points long\")\n",
    "\n",
    "    # Define and fit the TCN\n",
    "    model = TCNModel(\n",
    "        input_chunk_length = INPUT_CHUNK,\n",
    "        output_chunk_length = OUTPUT_CHUNK,\n",
    "        output_chunk_shift = 0,\n",
    "        likelihood = BernoulliLikelihood(), \n",
    "        n_epochs = N_EPOCHS,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        num_layers = NUM_LAYERS,\n",
    "        num_filters = NUM_FILTERS,\n",
    "        kernel_size = KERNEL_SIZE,\n",
    "        dilation_base = DILATION,\n",
    "        dropout = DROPOUT,\n",
    "        random_state = 42,\n",
    "    )\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.fit(series=train_targs,\n",
    "            past_covariates=train_feats,\n",
    "            val_series=val_targs,\n",
    "            val_past_covariates=val_feats,\n",
    "            max_samples_per_ts=5,\n",
    "            verbose=True)\n",
    "    fit_secs = time.time() - t0\n",
    "\n",
    "    # Evaluate\n",
    "    y_true, y_pred = [], []\n",
    "    for feats_ts, targ_ts in zip(test_feats_outer, test_targ_outer):\n",
    "        if len(targ_ts) < INPUT_CHUNK + OUTPUT_CHUNK:\n",
    "            continue  # nothing to predict safely\n",
    "        hist = targ_ts[:-OUTPUT_CHUNK]\n",
    "        future = targ_ts[-OUTPUT_CHUNK:] \n",
    "\n",
    "        preds = model.predict(n=OUTPUT_CHUNK,\n",
    "                              series=hist,\n",
    "                              past_covariates=feats_ts)\n",
    "\n",
    "        y_true.extend(future.values()[:, 0]) # first dim = burst_leak flag\n",
    "        y_pred.extend(preds .values()[:, 0])\n",
    "\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    pr_auc = average_precision_score(y_true, y_pred)\n",
    "    #nll = log_loss(y_true, y_pred)\n",
    "    brier = brier_score_loss(y_true, y_pred)\n",
    "\n",
    "    y_pred_bin = (np.asarray(y_pred) >= 0.5).astype(int)\n",
    "    precision = precision_score(y_true, y_pred_bin, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_bin, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred_bin, zero_division=0)\n",
    "    print(\n",
    "        f\"Fold {fold_no} | AUROC {auc:.4f}  PR-AUC {pr_auc:.4f}  \"\n",
    "        #f\"LogLoss {nll:.4f}  Brier {brier:.4f}\\n\"\n",
    "        f\"           Precision {precision:.3f}  Recall {recall:.3f}  F1 {f1:.3f}  \"\n",
    "        f\"(fit {fit_secs/60:.1f} min)\"\n",
    "    )\n",
    "    \n",
    "    total_houses = len(house_ids)  # Total houses in dataset\n",
    "    train_houses = len(train_idx)  # Houses used for training\n",
    "    test_houses = len(test_idx)    # Houses used for testing\n",
    "    \n",
    "    # Create timestamp and folder name with dataset info\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    folder_name = f\"{folder_timestamp}/{fold_no}\"\n",
    "    folder_path = f\"weights/{folder_name}\"\n",
    "    \n",
    "    # Create the folder\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(f\"{folder_path}/tcn_fold{fold_no}_{timestamp}.pt\")\n",
    "    \n",
    "    print(f\"Saved model to {folder_path}/tcn_fold{fold_no}_{timestamp}.pt\")\n",
    "\n",
    "    # Clean-up\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "    return {\n",
    "        \"fold\": fold_no,\n",
    "        \"AUROC\": auc,\n",
    "        \"PR_AUC\": pr_auc,\n",
    "        #\"LogLoss\": nll,\n",
    "        \"Brier\": brier,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1,\n",
    "        \"fit_seconds\": fit_secs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa42da0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_scaled, feature_scaler = scale_features_simple(ff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3054454",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "house_labels = np.array([int(ts.values()[:, 0].any()) for ts in house_target_series.values()])\n",
    "\n",
    "for fold_no, (train_idx, test_idx) in enumerate(\n",
    "        outer_cv.split(X=house_ids, y=house_labels, groups=house_ids), start=1):\n",
    "    print(f\"\\n===== OUTER FOLD {fold_no}/5 \"\n",
    "          f\"— train houses {len(train_idx)}, test houses {len(test_idx)} =====\")\n",
    "    fold_result = run_outer_fold(train_idx, test_idx, fold_no)\n",
    "    results.append(fold_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6af183",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n========== CV SUMMARY ==========\")\n",
    "summary_df = pd.DataFrame(results)\n",
    "display(summary_df)\n",
    "\n",
    "metric_cols = [\"AUROC\", \"PR_AUC\", #\"LogLoss\", \n",
    "                \"Brier\",\n",
    "               \"Precision\", \"Recall\", \"F1\"]\n",
    "\n",
    "mean_vals = summary_df[metric_cols].mean()\n",
    "print(\"Mean metrics across folds:\")\n",
    "for m, v in mean_vals.items():\n",
    "    print(f\" • {m:10s}: {v:.4f}\")\n",
    "\n",
    "# Save with all columns\n",
    "summary_df.to_json(\"cv_results_250.json\", orient=\"records\", indent=2)\n",
    "print(\"Saved per-fold metrics to cv_results_250.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74738e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WDN_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
